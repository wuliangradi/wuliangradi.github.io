<!DOCTYPE html>




<html class="theme-next muse" lang="zh-Hans">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />




  
  
  
  

  
    
    
  

  

  

  

  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.2" rel="stylesheet" type="text/css" />


  <meta name="keywords" content="Hexo, NexT" />








  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=5.1.2" />






<meta name="description" content="1. 概率论对于深度学习的意义   两方面的意义:构建算法和推理算法; 评估ai模型和结果.   这两句话总结的很好:概率论使得我们能对”不确定性”进行陈述和表达,并在这种”不确定性”的基础上做出推理;信息论使得我们能够量化概率分布不确定性.而机器学习必须处理不确定性量甚至是随机性量.   一个重要的准则是:”简单但是不确定”而不是”复杂但是确定”.机器学习语境下的概率常指可信度(degree o">
<meta property="og:type" content="article">
<meta property="og:title" content="概率论和信息论基础">
<meta property="og:url" content="wuliangradi.github.io/2017/05/15/2017-05-15-probability/index.html">
<meta property="og:site_name" content="旅途">
<meta property="og:description" content="1. 概率论对于深度学习的意义   两方面的意义:构建算法和推理算法; 评估ai模型和结果.   这两句话总结的很好:概率论使得我们能对”不确定性”进行陈述和表达,并在这种”不确定性”的基础上做出推理;信息论使得我们能够量化概率分布不确定性.而机器学习必须处理不确定性量甚至是随机性量.   一个重要的准则是:”简单但是不确定”而不是”复杂但是确定”.机器学习语境下的概率常指可信度(degree o">
<meta property="og:locale" content="zh-Hans">
<meta property="og:image" content="https://upload.wikimedia.org/wikipedia/commons/7/75/Binomial_distribution_pmf.svg">
<meta property="og:image" content="https://github.com/wuliangradi/images/raw/master/600px-Normal_distribution_pdf.svg.png">
<meta property="og:image" content="https://github.com/wuliangradi/images/raw/master/325px-Laplace_distribution_pdf.png">
<meta property="og:updated_time" content="2017-09-16T07:48:39.886Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="概率论和信息论基础">
<meta name="twitter:description" content="1. 概率论对于深度学习的意义   两方面的意义:构建算法和推理算法; 评估ai模型和结果.   这两句话总结的很好:概率论使得我们能对”不确定性”进行陈述和表达,并在这种”不确定性”的基础上做出推理;信息论使得我们能够量化概率分布不确定性.而机器学习必须处理不确定性量甚至是随机性量.   一个重要的准则是:”简单但是不确定”而不是”复杂但是确定”.机器学习语境下的概率常指可信度(degree o">
<meta name="twitter:image" content="https://upload.wikimedia.org/wikipedia/commons/7/75/Binomial_distribution_pmf.svg">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Muse',
    version: '5.1.2',
    sidebar: {"position":"left","display":"hide","offset":12,"offset_float":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":false,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="wuliangradi.github.io/2017/05/15/2017-05-15-probability/"/>





  <title>概率论和信息论基础 | 旅途</title>
  




<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
            (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
          m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
  ga('create', 'UA-106562186-1', 'auto');
  ga('send', 'pageview');
</script>





</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">旅途</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle">路过生命中漫无止境的寒冷和孤独</p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />
            
            关于
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="wuliangradi.github.io/2017/05/15/2017-05-15-probability/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Liang Wu">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="旅途">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">概率论和信息论基础</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2017-05-15T22:36:31+08:00">
                2017-05-15
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          
            <span class="post-meta-divider">|</span>
            <span class="page-pv"><i class="fa fa-file-o"></i> 浏览
            <span class="busuanzi-value" id="busuanzi_value_page_pv" ></span>次
            </span>
          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <h3 id="1-概率论对于深度学习的意义"><a href="#1-概率论对于深度学习的意义" class="headerlink" title="1. 概率论对于深度学习的意义"></a>1. 概率论对于深度学习的意义</h3><p>   两方面的意义:构建算法和推理算法; 评估ai模型和结果.<br>   这两句话总结的很好:概率论使得我们能对”不确定性”进行陈述和表达,并在这种”不确定性”的基础上做出推理;信息论使得我们能够量化概率分布不确定性.而机器学习必须处理不确定性量甚至是随机性量.<br>   一个重要的准则是:”简单但是不确定”而不是”复杂但是确定”.机器学习语境下的概率常指可信度(degree of belief),以即贝叶斯概率和频率概率的区别(这一段dl这本书说得很不严谨)</p>
<h3 id="2-概率分布"><a href="#2-概率分布" class="headerlink" title="2. 概率分布"></a>2. 概率分布</h3><h4 id="2-1-基本概念"><a href="#2-1-基本概念" class="headerlink" title="2.1 基本概念"></a>2.1 基本概念</h4><ul>
<li>随机变量有许多可能状态,每个状态对应不同的出现的可能性,描述各个可能状态出现的可能性大小的分布即为概率分布.   </li>
<li>概率质量函数（probability mass function，简写为pmf）是离散随机变量在各特定取值上的概率。概率质量函数和概率密度函数(probability desity function，简写为pdf)不同之处在于：概率密度函数是对连续随机变量定义的，本身不是概率，只有对连续随机变量的概率密度函数在某区间内进行积分后才是概率。   </li>
<li>联合概率分布,边缘概率分布.  </li>
<li>条件概率; 条件概率的链式法则; 独立性;</li>
<li>在给定有限次的观测前提下,对随机变量x的概率分布p(x)建模,这个问题被称为密度估计(density estimation)</li>
</ul>
<h4 id="2-2-常见概率分布"><a href="#2-2-常见概率分布" class="headerlink" title="2.2 常见概率分布"></a>2.2 常见概率分布</h4><ul>
<li>伯努利分布</li>
</ul>
<script type="math/tex; mode=display">
f(k;p)=p^{k}(1-p)^{1-k}\!\quad {\text{for }}k\in \{0,1\}</script><p>伯努利分布的情形下,通过最大似然函数来估计u参数值,得到最大似然估计值为:正面朝上的概率等于观测到的数据集正面朝上的观测所占的比例.最大似然容易产生过拟合,所以最好引入参数u的先验分布.</p>
<ul>
<li>二项分布(Binomial distribution)<br>概念:给定数据集规模N的条件下,x=1观测出现的数量为k的概率分布,被称为二项分布. 二项分布即为重复n次的伯努利试验.</li>
</ul>
<script type="math/tex; mode=display">
f(k;n,p)=\Pr(X=k)={\binom {n}{k}}p^{k}(1-p)^{n-k}</script><script type="math/tex; mode=display">
{\binom {n}{k}}={\frac {n!}{k!(n-k)!}}</script><p>二项分布和总数和概率相关,如下图所示:   </p>
<p><img src="https://upload.wikimedia.org/wikipedia/commons/7/75/Binomial_distribution_pmf.svg" alt=""></p>
<ul>
<li>Beta分布(Beta distribution)<br>引入关于参数u的先验分布.首先是连续概率分布. 知乎的问题回答得比较生动说明了<a href="https://www.zhihu.com/question/30269898" target="_blank" rel="external">如何通俗理解beta分布</a>. beta分布可以看作一个概率的概率分布</li>
</ul>
<script type="math/tex; mode=display">
{\displaystyle {\begin{aligned}f(x;\alpha ,\beta )&=\mathrm {constant} \cdot x^{\alpha -1}(1-x)^{\beta -1}\\[3pt]&={\frac {x^{\alpha -1}(1-x)^{\beta -1}}{\displaystyle \int _{0}^{1}u^{\alpha -1}(1-u)^{\beta -1}\,du}}\\[6pt]&={\frac {\Gamma (\alpha +\beta )}{\Gamma (\alpha )\Gamma (\beta )}}\,x^{\alpha -1}(1-x)^{\beta -1}\\[6pt]&={\frac {1}{\mathrm {B} (\alpha ,\beta )}}x^{\alpha -1}(1-x)^{\beta -1}\end{aligned}}}</script><p>相当于α和β是形状修正因子(都是超参数),而前面的常数(归一化常数)是为了共轭做的修正,保证Beta分布式归一化.共轭保证先验分布是beta分布,后验分布还是Beta分布</p>
<script type="math/tex; mode=display">
\operatorname{E}[X] = \frac{\alpha}{\alpha+\beta}\!</script><script type="math/tex; mode=display">
\operatorname{var}[X] = \frac{\alpha\beta}{(\alpha+\beta)^2(\alpha+\beta+1)}\!</script><ul>
<li>Beta二项分布(Beta-binomial distribution)</li>
</ul>
<script type="math/tex; mode=display">
{\displaystyle {\begin{aligned}
P(\theta |data) \propto \theta^m(1-\theta)^{l}*\theta^{a-1}(1-\theta)^{b-1} \\
\propto  \theta^{a+m-1}(1-\theta)^{b+l-1}
\end{aligned} } }</script><p>从先验概率到后验概率,α和β的值都变大了m和l,那么我们可以将α和β理解为x=1和x=0的有效观测数.接下来增加观测,后验概率的分布可以转变成先验分布的角色.随着观测的数据越来越多,后验概率表示的不确定性将会持续下降.</p>
<ul>
<li>多项式分布(multinomial distribution)<br>多项式分布是二项分布的扩展,对”k种可能,每种可能有着各自固定的出现概率,n次独立试验”的概率分布.当k是2,n是0的多项式分布是伯努利分布;当k是2,n是超过1的试验次数的多项式分布是二项式分布;当n是1的多项式分布是分类分布.   <blockquote>
<p>The Bernoulli distribution is the probability distribution of whether a Bernoulli trial is a success. In other words, it models the number of heads from flipping a (possibly biased) coin one time. The binomial distribution generalizes this to the number of heads from doing n independent flips of the same coin. For the multinomial distribution the analog to the Bernoulli Distribution is the categorical distribution. Instead of flipping one coin, the categorical distribution models the roll of one k sided die. So the multinomial distribution can model n independent rolls of a k sided die</p>
</blockquote>
</li>
</ul>
<script type="math/tex; mode=display">
{\displaystyle {\begin{aligned}f(x_{1},\ldots ,x_{k};n,p_{1},\ldots ,p_{k})&{}=\Pr(X_{1}=x_{1}{\text{ and } }\dots {\text{ and }}X_{k}=x_{k})\\&{}={\begin{cases}{\displaystyle {n! \over x_{1}!\cdots x_{k}!}p_{1}^{x_{1} }\cdots p_{k}^{x_{k} } },\quad &{\text{when } }\sum _{i=1}^{k}x_{i}=n\\\\0&{\text{otherwise,} }\end{cases}}\end{aligned} } }</script><p>x表示观测到该值出现的次数</p>
<ul>
<li>狄利克雷分布(Dirichlet distribution)<br>狄利克雷分布是Beta分布的多元扩展,是贝叶斯统计中常见的先验分布,是分类分布和多项式分布的共轭先验.</li>
</ul>
<script type="math/tex; mode=display">
{\displaystyle f\left(x_{1},\ldots ,x_{K-1};\alpha _{1},\ldots ,\alpha _{K}\right)={\frac {1}{\mathrm {B} ({\boldsymbol {\alpha }})}}\prod _{i=1}^{K}x_{i}^{\alpha _{i}-1},}</script><script type="math/tex; mode=display">
{\displaystyle \mathrm {B} ({\boldsymbol {\alpha }})={\frac {\prod _{i=1}^{K}\Gamma (\alpha _{i})}{\Gamma \left(\sum _{i=1}^{K}\alpha _{i}\right)}},\qquad {\boldsymbol {\alpha }}=(\alpha _{1},\ldots ,\alpha _{K}).}</script><ul>
<li>高斯分布(Gaussian distribution)<br><img src="https://github.com/wuliangradi/images/raw/master/600px-Normal_distribution_pdf.svg.png" alt=""><br>高斯分布也称为正态分布,一元变量x和D维向量x的高斯分布形式如下:</li>
</ul>
<script type="math/tex; mode=display">
{\displaystyle f(x\;|\;\mu ,\sigma ^{2})={\frac {1}{\sqrt {2\pi \sigma ^{2} } } }\;e^{-{\frac {(x-\mu )^{2}}{2\sigma ^{2}} } } }</script><script type="math/tex; mode=display">
{\displaystyle {\begin{aligned}f_{\mathbf {x} }(x_{1},\ldots ,x_{k})&={\frac {\exp \left(-{\frac {1}{2}}({\mathbf {x} }-{\boldsymbol {\mu }})^{\mathrm {T} }{\boldsymbol {\Sigma }}^{-1}({\mathbf {x} }-{\boldsymbol {\mu }})\right)}{\sqrt {(2\pi )^{k}|{\boldsymbol {\Sigma }}|}}}\\[5pt]&={\frac {\exp \left(-{\frac {1}{2}}({\mathbf {x} }-{\boldsymbol {\mu }})^{\mathrm {T} }{\boldsymbol {\Sigma }}^{-1}({\mathbf {x} }-{\boldsymbol {\mu }})\right)}{\sqrt {|2\pi {\boldsymbol {\Sigma }}|}}},\end{aligned}}}</script><p>   高斯分布的缺点:    </p>
<pre><code>- 参数随着唯独的增长而呈平方的方式增长,解决办法:使用协方差矩阵的限制形式,协方差矩阵-对角的协方差矩阵-各向同性协方差矩阵   
- 高斯分布是单峰的,因此不能很好的近似多峰分布,解决办法:引入潜在变量,相当多的多峰分布可以使用混合高斯分布来描述    
</code></pre><ul>
<li>条件高斯分布和边缘高斯分布</li>
<li>高斯变量的贝叶斯定理</li>
</ul>
<ul>
<li>泊松分布(Poisson distribution)和指数分布(Exponential distribution)<br>泊松分布的参数λ是单位时间（或单位面积）内随机事件的平均发生率<script type="math/tex; mode=display">
P(X=k)=\frac{e^{-\lambda}\lambda^k}{k!}</script></li>
</ul>
<p>其中λ &gt; 0是分布的一个参数，常被称为率参数(rate parameter)。即每单位时间发生该事件的次数<br>指数分布的概率密度函数是</p>
<script type="math/tex; mode=display">
f(x;\lambda) = \left\{\begin{matrix}
\lambda e^{-\lambda x} &,\; x \ge 0, \\
0 &,\; x < 0.
\end{matrix}\right.</script><ul>
<li><p>Laplace分布<br>随机变量的概率密度函数分布为</p>
<script type="math/tex; mode=display">
f(x|\mu,b) = \frac{1}{2b} \exp \left( -\frac{|x-\mu|}{b} \right) \</script><p><img src="https://github.com/wuliangradi/images/raw/master/325px-Laplace_distribution_pdf.png" alt=""></p>
</li>
<li><p>分布的混合</p>
<ul>
<li>高斯混合模型</li>
</ul>
</li>
</ul>
<h3 id="3-期望-方差和协方差"><a href="#3-期望-方差和协方差" class="headerlink" title="3. 期望,方差和协方差"></a>3. 期望,方差和协方差</h3><script type="math/tex; mode=display">
   \mu =\sum _{i=1}^{n}p_{i}\cdot f(x_{i}).</script><script type="math/tex; mode=display">
   \mu =\int p(x)\,f(x)\,dx\,</script><script type="math/tex; mode=display">
   {\displaystyle {\begin{aligned}\operatorname {Var} (f(x))&=\operatorname {E} \left[(f(x)-\operatorname {E} [f(x)])^{2}\right]\\&\end{aligned} } }</script><script type="math/tex; mode=display">
   \operatorname {cov} (f(x),g(y))=\operatorname {E} { {\big [}(f(x)-\operatorname {E} [f(x)])(g(y)-\operatorname {E} [g(y)]){\big ]} },</script><h3 id="3-贝叶斯视角"><a href="#3-贝叶斯视角" class="headerlink" title="3. 贝叶斯视角"></a>3. 贝叶斯视角</h3><h3 id="4-Maximum-likelihood-estimation"><a href="#4-Maximum-likelihood-estimation" class="headerlink" title="4. Maximum likelihood estimation"></a>4. Maximum likelihood estimation</h3><h4 id="4-1-基本概念"><a href="#4-1-基本概念" class="headerlink" title="4.1 基本概念"></a>4.1 基本概念</h4><p>In statistics, maximum likelihood estimation (MLE) is a method of estimating the parameters of a statistical model given observations, by finding the parameter values that maximize the likelihood of making the observations given the parameters<br>固定的数据 潜在的统计Christopher M Bishop模型 最大似然选择参数值 以最大化似然函数<br>maximizes the “agreement”  </p>
<h4 id="4-2-准则"><a href="#4-2-准则" class="headerlink" title="4.2 准则"></a>4.2 准则</h4><p>独立同分布的观测值 参数模型 估计这个模型的参数<br>数据是事实了 如何使用最合理的参数使得已经成为事实的数据最大概率的出现  </p>
<script type="math/tex; mode=display">
f(x_{1},x_{2},\ldots ,x_{n}\mid \theta )=f(x_{1}\mid \theta )\times f(x_{2}|\theta )\times \cdots \times f(x_{n}\mid \theta )</script><script type="math/tex; mode=display">
{\mathcal {L}}(\theta \,;\,x_{1},\ldots ,x_{n})=f(x_{1},x_{2},\ldots ,x_{n}\mid \theta )=\prod _{i=1}^{n}f(x_{i}\mid \theta ).</script><p>对数似然,再或者是对数似然的均值<br>参看维基百科对<a href="https://en.wikipedia.org/wiki/Maximum_likelihood_estimation" target="_blank" rel="external">最大似然函数</a>的解释</p>
<script type="math/tex; mode=display">
\ln {\mathcal {L}}(\theta \,;\,x_{1},\ldots ,x_{n})=\sum _{i=1}^{n}\ln f(x_{i}\mid \theta ),</script><p>Christopher M Bishop</p>
<p>最大似然估计和最大后验估计(Maximum a posteriori estimation<br>MAP)不谋而合  </p>
<script type="math/tex; mode=display">
P(\theta \mid x_{1},x_{2},\ldots ,x_{n})={\frac {f(x_{1},x_{2},\ldots ,x_{n}\mid \theta )P(\theta )}{P(x_{1},x_{2},\ldots ,x_{n})}}</script><h3 id="5-Maximum-a-posteriori-estimation"><a href="#5-Maximum-a-posteriori-estimation" class="headerlink" title="5. Maximum a posteriori estimation"></a>5. Maximum a posteriori estimation</h3><p>最大后验概率估计与最大似然估计中的经典方法有密切关系，但是它使用了一个增广的优化目标，进一步考虑了被估计量的先验概率分布。所以最大后验概率估计可以看作是规则化（regularization）的最大似然估计。<br><strong><em>考虑了被估计量的先验分布</em></strong><br>这样f(θ)就是总体参数为θ时x的概率<br>函数  </p>
<script type="math/tex; mode=display">
\theta \mapsto f(x|\theta )\!</script><p>就是似然函数<br>估计就是θ的最大似然估计: </p>
<script type="math/tex; mode=display">
{\hat  {\theta } }_{ { {\mathrm  {ML} } } }(x)=\arg \max _{ {\theta } }f(x|\theta )\!</script><p>假设θ存在一个先验分布g,这样θ的后验分布就是:  </p>
<script type="math/tex; mode=display">
\theta \mapsto {\frac  {f(x|\theta )\,g(\theta )}{\int _{ {\Theta } }f(x|\theta ')\,g(\theta ')\,d\theta '} }\!</script><p>最大后延估计就是估计θ为这个随机变量的后验分布的众数:  </p>
<script type="math/tex; mode=display">
{\hat  {\theta } }_{ { {\mathrm  {MAP} } } }(x)=\arg \max _{ {\theta } }{\frac  {f(x|\theta )\,g(\theta )}{\int _{ {\Theta } }f(x|\theta ')\,g(\theta ')\,d\theta '} }=\arg \max _{ {\theta } }f(x|\theta )\,g(\theta )\!</script><p>最大后验估计是点估计,并不属于贝叶斯方法</p>
<h3 id="6-KL散度"><a href="#6-KL散度" class="headerlink" title="6. KL散度"></a>6. KL散度</h3><h4 id="6-1-概念"><a href="#6-1-概念" class="headerlink" title="6.1 概念"></a>6.1 概念</h4><p>The Kullback–Leibler divergence是一种概率分布和期望的概率分布的偏离的测量<br>在贝叶斯视角，是先验概率分布Q和后验概率分布P的信息增益.P代表数据和观测值的真实分布。<br>机器学习语境，KL散度常被称为信息增益，或者是分布P相对于分布Q的相对熵(relative entropy).在对未知概率分布 建模时,使用了不同于真实真实的概率分布,会损失编码效率,额外的信息量即为两个分布之间的KL散度.</p>
<h4 id="6-2-形式Christopher-M-Bishop"><a href="#6-2-形式Christopher-M-Bishop" class="headerlink" title="6.2 形式Christopher M Bishop"></a>6.2 形式Christopher M Bishop</h4><p>对于离散概率分布P和Q</p>
<script type="math/tex; mode=display">
D_{\mathrm {KL} }(P\|Q)=\sum _{i}P(i)\,\log {\frac {P(i)}{Q(i)}}.</script><p>也就是两个概率log差的期望</p>
<p>对于连续概率分布P和Q</p>
<script type="math/tex; mode=display">
D_{\mathrm {KL} }(P\|Q)=\int _{-\infty }^{\infty }p(x)\,\log {\frac {p(x)}{q(x)}}\,{\rm {d}}x,\!</script><p>更一般地</p>
<script type="math/tex; mode=display">
D_{\mathrm {KL} }(P\|Q)=\int _{X}\log {\frac { {\rm {d} }P}{ {\rm {d} }Q} }\,{\rm {d} }P,\!</script><h4 id="6-3-信息论的视角"><a href="#6-3-信息论的视角" class="headerlink" title="6.3 信息论的视角"></a>6.3 信息论的视角</h4><p>信息论:计量信号中多少信息被表现出来.量化信息的两个准则:</p>
<ul>
<li>不太可能发生的事件发生提供更多的信息,信息内容的度量依赖于概率分布p(x),信息的表示函数应该是概率的单调递增函数   </li>
<li>独立的事件有额外的信息,两个独立事件同时发生的信息应该等于事件各自发生获得的信息之和<br>此时应该有:</li>
</ul>
<script type="math/tex; mode=display">
h(x, y)=h(x)+h(y)</script><script type="math/tex; mode=display">
p(x,y)=p(x)p(y)</script><p>   信息量h(x)的定义满足上面两个准则:</p>
<script type="math/tex; mode=display">
h(x)=\log _{2}p(x)</script><p>垫付（611.00元）</p>
<p>   香农熵则定义为随机变量的信息量的期望,如以下公式所示(分别是连续随机变量和离散随机变量的熵的表达形式).当然标准的香农熵应该以2为对数的底,此时熵的单位是比特.我们常用自然对数,此时熵的单位是nat.非均匀分布均匀分布的熵要小,熵是传输一个随机变量状态值所需的比特位的下界.</p>
<script type="math/tex; mode=display">
\mathrm {H} (X)=\mathrm {E} [\mathrm {I} (X)]=\mathrm {E} [-\ln(\mathrm {P} (X))].</script><script type="math/tex; mode=display">
{\displaystyle \mathrm {H} (X)=\sum _{i=1}^{n}{\mathrm {P} (x_{i})\,\mathrm {I} (x_{i})}=-\sum _{i=1}^{n}{\mathrm {P} (x_{i})\log _{b}\mathrm {P} (x_{i})},}</script><script type="math/tex; mode=display">
h(X) = -\int_\mathbb{X} f(x)\log f(x)\,dx</script><p>最大化微分熵的分布是高斯分布,高斯分布的微分熵如下所示;熵的大小随着分布的宽度即方差的增加而增加,微分熵可以为负.各类分布的微分熵可以详见维基百科词条<a href="https://en.wikipedia.org/wiki/Differential_entropy" target="_blank" rel="external">微分熵</a></p>
<script type="math/tex; mode=display">
\ln\left(\sigma\sqrt{2\,\pi\,e}\right)</script><p>KL散度正是两个分布的<strong><em>相对熵</em></strong></p>
<h4 id="6-4-KL散度的特点"><a href="#6-4-KL散度的特点" class="headerlink" title="6.4 KL散度的特点"></a>6.4 KL散度的特点</h4><ul>
<li>非负, P和Q是相同的分布,则KL散度是0</li>
<li><p>非对称的</p>
<script type="math/tex; mode=display">
D_{\mathrm {KL} }(P\|Q) \neq D_{\mathrm {KL} }(Q\|P)\,\!</script><p> 正是因为P和Q分布不一致,才导致了两者的KL散度不一致</p>
</li>
</ul>
<h3 id="7-交叉熵-条件熵"><a href="#7-交叉熵-条件熵" class="headerlink" title="7. 交叉熵,条件熵"></a>7. 交叉熵,条件熵</h3><script type="math/tex; mode=display">
H(p,q)=\operatorname {E}_{p}[-\log q]=H(p)+D_{ { {\mathrm  {KL} } } }(p\|q)\!</script><p>最小化交叉熵等价于最小化KL散度</p>
<p>离散的情况  </p>
<script type="math/tex; mode=display">
H(p,q)=-\sum _{x}p(x)\,\log q(x)\</script><h3 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h3><ul>
<li><a href="http://www.deeplearningbook.org/" target="_blank" rel="external">Deep Learning, Ian Goodfellow et al;</a></li>
<li>Pattern Recognition and Machine Learning; Christopher M Bishop</li>
<li>维基百科<a href="https://en.wikipedia.org/wiki/Differential_entropy" target="_blank" rel="external">Differential entropy</a>    </li>
<li>维基百科<a href="https://en.wikipedia.org/wiki/Entropy_\(information_theory\" target="_blank" rel="external">Entropy</a>)    </li>
<li>维基百科<a href="https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence" target="_blank" rel="external">Kullback–Leibler divergence</a>    </li>
<li>维基百科<a href="https://en.wikipedia.org/wiki/Cross_entropy" target="_blank" rel="external">Cross_entropy</a>   </li>
<li>维基百科<a href="https://en.wikipedia.org/wiki/Multinomial_distribution" target="_blank" rel="external">multinomial distribution</a>    </li>
<li>维基百科<a href="https://en.wikipedia.org/wiki/Dirichlet_distribution" target="_blank" rel="external">Dirichlet distribution</a></li>
</ul>

      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2017/05/13/2017-05-13-logistic regression/" rel="next" title="机器学习第二层之逻辑回归">
                <i class="fa fa-chevron-left"></i> 机器学习第二层之逻辑回归
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2017/06/08/2017-06-08-ML/" rel="prev" title="机器学习基础">
                机器学习基础 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          
  <div class="comments" id="comments">
    
  </div>


        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap" >
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          
            <p class="site-author-name" itemprop="name">Liang Wu</p>
            <p class="site-description motion-element" itemprop="description">--</p>
        </div>

        <nav class="site-state motion-element">

          
            <div class="site-state-item site-state-posts">
            
              <a href="/archives/">
            
                <span class="site-state-item-count">24</span>
                <span class="site-state-item-name">日志</span>
              </a>
            </div>
          

          

          

        </nav>

        

        <div class="links-of-author motion-element">
          
        </div>

        
        

        
        

        


      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-概率论对于深度学习的意义"><span class="nav-number">1.</span> <span class="nav-text">1. 概率论对于深度学习的意义</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-概率分布"><span class="nav-number">2.</span> <span class="nav-text">2. 概率分布</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#2-1-基本概念"><span class="nav-number">2.1.</span> <span class="nav-text">2.1 基本概念</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-2-常见概率分布"><span class="nav-number">2.2.</span> <span class="nav-text">2.2 常见概率分布</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-期望-方差和协方差"><span class="nav-number">3.</span> <span class="nav-text">3. 期望,方差和协方差</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-贝叶斯视角"><span class="nav-number">4.</span> <span class="nav-text">3. 贝叶斯视角</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-Maximum-likelihood-estimation"><span class="nav-number">5.</span> <span class="nav-text">4. Maximum likelihood estimation</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#4-1-基本概念"><span class="nav-number">5.1.</span> <span class="nav-text">4.1 基本概念</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#4-2-准则"><span class="nav-number">5.2.</span> <span class="nav-text">4.2 准则</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-Maximum-a-posteriori-estimation"><span class="nav-number">6.</span> <span class="nav-text">5. Maximum a posteriori estimation</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#6-KL散度"><span class="nav-number">7.</span> <span class="nav-text">6. KL散度</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#6-1-概念"><span class="nav-number">7.1.</span> <span class="nav-text">6.1 概念</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#6-2-形式Christopher-M-Bishop"><span class="nav-number">7.2.</span> <span class="nav-text">6.2 形式Christopher M Bishop</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#6-3-信息论的视角"><span class="nav-number">7.3.</span> <span class="nav-text">6.3 信息论的视角</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#6-4-KL散度的特点"><span class="nav-number">7.4.</span> <span class="nav-text">6.4 KL散度的特点</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#7-交叉熵-条件熵"><span class="nav-number">8.</span> <span class="nav-text">7. 交叉熵,条件熵</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#参考资料"><span class="nav-number">9.</span> <span class="nav-text">参考资料</span></a></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright" >
  
  &copy; 
  <span itemprop="copyrightYear">2017</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Liang Wu</span>

  
</div>


  <div class="powered-by">由 <a class="theme-link" href="https://hexo.io">Hexo</a> 强力驱动</div>

  <span class="post-meta-divider">|</span>

  <div class="theme-info">主题 &mdash; <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">NexT.Muse</a> v5.1.2</div>


        
<div class="busuanzi-count">
  <script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>

  
    <span class="site-uv">
      <i class="fa fa-user"></i> 访问人数
      <span class="busuanzi-value" id="busuanzi_value_site_uv"></span>
      
    </span>
  

  
    <span class="site-pv">
      <i class="fa fa-eye"></i> 总访问量
      <span class="busuanzi-value" id="busuanzi_value_site_pv"></span>
      次
    </span>
  
</div>








        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>

  
  <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.2"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.2"></script>



  
  

  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.2"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.2"></script>


  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.2"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.2"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.2"></script>



  


  




	





  





  








  





  

  

  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


  

  

</body>
</html>
